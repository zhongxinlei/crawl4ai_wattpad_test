import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig, LXMLWebScrapingStrategy
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

from time import sleep
import random
import requests
from bs4 import BeautifulSoup
import os
import re

link_url = "https://www.wattpad.com/story/21067756-enhancement"
novel_name = "enhancement"
output_path = f"link_folder/{novel_name}_output.jsonl"
jsonl_path = f"link_folder/{novel_name}_output.jsonl"
raw_novel_path = "raw_novel/"


def fetch_and_save_content(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }

    try:
        # å‘èµ· HTTP è¯·æ±‚è·å–ç½‘é¡µå†…å®¹
        random_sleep_time = random.uniform(1, 2)
        # sleep(random_sleep_time)
        print(f"start to fetch: {url}")
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        html_content = response.text

        # ä½¿ç”¨ BeautifulSoup è§£æ HTML
        soup = BeautifulSoup(html_content, 'html.parser')

        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª <pre> æ ‡ç­¾
        parents = soup.find_all(class_=lambda c: c and 'page' in c and 'highlighter' in c)
        # p_tags = soup.select('.page.highlighter p')
        all_p_contents = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰ <p> æ ‡ç­¾çš„å†…å®¹
        content = ''
        for parent in parents:
            all_p_contents.extend(parent.find_all('p'))
        for p_tag in all_p_contents:
            # æå– <p> æ ‡ç­¾çš„çº¯æ–‡æœ¬å†…å®¹
            text = p_tag.get_text(strip=True)
            if text:  # ç¡®ä¿å†…å®¹éç©º
                content = content + '\n\n' + text

        # å¦‚æœæœ‰å†…å®¹ï¼Œåˆ™å†™å…¥åˆ°æ–‡ä»¶ä¸­
        if content:
           return content
        else:
            print(f"No content found for {url}")
            return None
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None
    except Exception as e:
        print(f"Error processing {url}: {e}")
        return None


async def extract_crypto_prices():
    # 1. Define a simple extraction schema
    schema = {
        "name": "chapter links",
        "baseSelector": "div._01L-d ul[aria-label='story-parts'] > li",    # Repeated elements
        "fields": [
            {
                "name": "chapter_name",
                "selector": "div.wpYp-",
                "type": "text"
            },
            {
                "name": "chapter_link",
                "selector": "a._6qJpE",
                "type": "attribute",
                "attribute": "href"
            }
        ]
    }

    # 2. Create the extraction strategy
    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    # 3. Set up your crawler config (if needed)
    config = CrawlerRunConfig(
        # e.g., pass js_code or wait_for if the page is dynamic
        # wait_for="css:.crypto-row:nth-child(20)"
        cache_mode = CacheMode.BYPASS,
        extraction_strategy=extraction_strategy,
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        # 4. Run the crawl and extraction
        result = await crawler.arun(
            url=link_url,

            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return

        # 5. Parse the extracted JSON
        data = json.loads(result.extracted_content)
        print(f"Extracted {len(data)} chapters")
        print(json.dumps(data[0], indent=2) if data else "No data found")
        with open(output_path, "w", encoding="utf-8") as f:
            if isinstance(data, list):
                # å¦‚æœ data æ˜¯åˆ—è¡¨ï¼Œéå†æ¯ä¸ªå¯¹è±¡
                for item in data:
                    json_line = json.dumps(item, ensure_ascii=False)
                    f.write(json_line + "\n")
            elif isinstance(data, dict):
                # å¦‚æœ data æ˜¯å­—å…¸ï¼Œç›´æ¥å†™å…¥å•è¡Œ
                json_line = json.dumps(data, ensure_ascii=False)
                f.write(json_line + "\n")
            else:
                raise ValueError("Unsupported data type. Expected list or dict.")


async def crawl_novel(chapter_name, url):
    """å¼‚æ­¥çˆ¬å–å°è¯´å†…å®¹"""
    print(f"\nğŸ” å¼€å§‹çˆ¬å–: {chapter_name} - {url}")

    try:
        schema = {
            "name": "chapter links",
            "baseSelector": "div.page.highlighter p",  # Repeated elements
            "fields": [
                {
                    "name": "chapter_content",
                    # "selector": "p",
                    "type": "text"
                },
                {
                    "name": "chapter_id",
                    # "selector": "p",
                    "type": "attribute",
                    "attribute": "data-p-id"
                }
            ]
        }

        # 2. Create the extraction strategy
        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
        xml_extraction_strategy = LXMLWebScrapingStrategy(schema)

        # Configure browser settings
        browser_config = BrowserConfig(
            headless=True
        )

        # Configure crawler settings
        crawler_run_config = CrawlerRunConfig(
            scan_full_page=True,
            scroll_delay=2.2,
            wait_for_images=True,
            cache_mode=CacheMode.BYPASS,
            extraction_strategy=extraction_strategy,
            page_timeout=180000,
            # wait_for=".recommended-stories-view"
        )

        # 3. Set up your crawler config (if needed)
        config = CrawlerRunConfig(
            # e.g., pass js_code or wait_for if the page is dynamic
            # wait_for="css:.crypto-row:nth-child(20)"
            cache_mode=CacheMode.BYPASS,
            extraction_strategy=extraction_strategy,
            wait_for=".recommended-stories-view",
            page_timeout=180000
        )
        async with AsyncWebCrawler(verbose=True) as crawler:
            # 4. Run the crawl and extraction
            result = await crawler.arun(
                url=url,
                config=crawler_run_config
            )

            if not result.success:
                print("Crawl failed:", result.error_message)
                return None

            # 5. Parse the extracted JSON
            data = json.loads(result.extracted_content)
            print(f"Extracted {len(data)} chapters")
            print(json.dumps(data[0], indent=2) if data else "No data found")
            if len(data) < 50 and chapter_name != 'Chapter 200':
                print(f'data length less than 50! need double check: {chapter_name}')
            # else:
            content = ''
            for p_tag in data:
                text = p_tag.get('chapter_content')
                if text:  # ç¡®ä¿å†…å®¹éç©º
                    if content == '':
                        content = text
                    else:
                        content = content + '\n\n' + text
            # å†™å…¥å†…å®¹åˆ°æ–‡ä»¶
            output_file_path = os.path.join(raw_novel_path, clean_chapter_name(chapter_name))
            with open(output_file_path, 'w', encoding='utf-8') as outfile:
                outfile.write(content + '\n\n')  # å†™å…¥å†…å®¹å¹¶æ·»åŠ ç©ºè¡Œåˆ†æ®µ
                print(f"Content saved to {output_file_path}")

            return None
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
        return None


def clean_chapter_name(s: str):
    if ':' in s:
        return s.split(':', 1)[0]
    else:
        return s

def extract_chapters(jsonl_path):
    """
    ä»JSONLæ–‡ä»¶æå–ç« èŠ‚ä¿¡æ¯
    å‚æ•°ï¼š
        jsonl_path: JSONLæ–‡ä»¶è·¯å¾„
    è¿”å›ï¼š
        åŒ…å«(chapter_name, chapter_link)å…ƒç»„çš„åˆ—è¡¨
    """
    results = []

    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_number, line in enumerate(f, 1):
            # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
            stripped_line = line.strip()

            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
            if not stripped_line or stripped_line.startswith('//'):
                continue

            try:
                # è§£æJSON
                data = json.loads(stripped_line)

                # æå–ç›®æ ‡å­—æ®µ
                name = data.get('chapter_name')
                link = data.get('chapter_link')

                # éªŒè¯å­—æ®µå­˜åœ¨æ€§
                if not name or not link:
                    print(f"è­¦å‘Šï¼šç¬¬{line_number}è¡Œç¼ºå°‘å¿…è¦å­—æ®µ")
                    continue

                results.append((name, link))

            except json.JSONDecodeError:
                print(f"é”™è¯¯ï¼šç¬¬{line_number}è¡ŒJSONè§£æå¤±è´¥")
            except Exception as e:
                print(f"æ„å¤–é”™è¯¯ï¼ˆç¬¬{line_number}è¡Œï¼‰: {str(e)}")

    return results

if __name__ == "__main__":
    # asyncio.run(extract_crypto_prices())

    chapter_links = extract_chapters(jsonl_path)
    for chapter_link in chapter_links:
        print(chapter_link[0] + '->' + chapter_link[1])
        asyncio.run(crawl_novel(chapter_link[0],chapter_link[1]))

        # fetch_and_save_content('https://www.wattpad.com/1310315593-dungeon-diver-stealing-a-monster%27s-power-chapter-1')
