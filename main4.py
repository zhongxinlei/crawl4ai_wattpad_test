import csv
import spacy
from tqdm.asyncio import tqdm_asyncio
from transformers import pipeline
from googletrans import Translator
import torch
import os
from litellm import completion
import asyncio
from pathlib import Path
from  utuil import *
from flair.data import Sentence
from flair.models import SequenceTagger
from transformers import AutoModelForCausalLM, AutoTokenizer

raw_novel_path = 'raw_novel_test/'
raw_translated_path = 'translated/'
os.environ['LITELLM_LOG'] = 'DEBUG'
comic_descriptions_chunk_size = 700
polish_chunk_size = 1500

class WattpadProcessor:
    def __init__(self):

        self.translations = self._load_translations()

        # åˆå§‹åŒ–NLPæ¨¡å‹
        # self.tagger = SequenceTagger.load("flair/ner-english-large")

        # model_name_or_path = "tencent/HY-MT1.5-7B"
        # self.translator_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        # self.translator_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto")


    def _load_translations(self):
        """åŠ è½½å·²æœ‰çš„ç¿»è¯‘è®°å¿†"""
        translations = {}
        try:
            with open('names/translations.csv', 'r') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) >= 2:
                        translations[row[0]] = row[1]
        except FileNotFoundError as e:
            print(f'_load_translations error! {str(e)}')
            pass
        return translations


    async def process_entities(self, text):
        """å¤„ç†å®ä½“è¯†åˆ«å’Œç¿»è¯‘"""
        # doc = self.nlp(text)
        sentence = Sentence(text)
        self.tagger.predict(sentence)
        new_translations = {}

        for entity in sentence.get_spans('ner'):
            # æ›´æ–°ç¿»è¯‘è®°å¿†åº“
            ent_text = unicode_clean(entity.text)
            ent_label = unicode_clean(entity.tag)
            tag_text = ent_label + '->' + ent_text
            if tag_text not in self.translations:
                zh_name = await self._translate_name(ent_text)
                self.translations[tag_text] = zh_name
                new_translations[tag_text] = zh_name

            # ä¿å­˜äººç‰©æè¿°
            # if ent.label_ == 'PERSON':
            #     await self._save_character_description(ent_text, text)

        # ä¿å­˜æ–°å¢ç¿»è¯‘
        if new_translations:
            self._save_translations(new_translations)

        return self.translations

    async def _translate_name(self, name):
        """ç¿»è¯‘ä¸“æœ‰åè¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        messages = [
            {"role": "user",
             "content": f"Translate the following segment into Chinese, without additional explanation.\n\n{name}"},
        ]
        try:
            tokenized_chat = self.translator_tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=False,
                return_tensors="pt"
            )
            outputs = self.translator_model.generate(tokenized_chat.to(self.translator_model.device), max_new_tokens=2048)
            translated_name = self.translator_tokenizer.decode(outputs[0])
            return process_tencent_HY(translated_name)
        except Exception as e:
            print(f'_translate_name exception: {e}')
            return name

    async def _save_character_description(self, name, context):
        """ä¿å­˜äººç‰©æè¿°"""
        desc = await self._extract_description(context, name)
        text = ''
        with open('names/character_descriptions.txt', 'r') as f:
            text = f.read()
        if txt_contains_no_name_sentence(text, f'[{name}]'):
            with open('names/character_descriptions.txt', 'a') as f2:
                print(f'_save_character_description: {desc}')
                desc = desc.replace('\n', ';')
                f2.write(f"[{name}] {desc}\n")

    async def _extract_description(self, text, name):
        """æå–äººç‰©æè¿°ä¸Šä¸‹æ–‡"""
        # å®šä¹‰æç¤ºè¯

        prompt = f"""
            you are a expert in charactor extraction, please extract the detail description of {name} in the given text,
            including it's appearance, dressing and charactor.
            output shall be a full sentence and shall be less then 100 letters.
            textï¼š
            {text}
            """
        # api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
        # api_key = "sk-b12ef3f919ab4f54ae1905e645762a40", api_base = "https://api.deepseek.com/v1", model = "deepseek/deepseek-chat", max_tokens=8192
        description = await self._call_llm_api(
            prompt,
            api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
        )
        return description if description else "no desc"

    def _save_translations(self, new_translations):
        """ä¿å­˜æ–°å¢ç¿»è¯‘"""
        with open('names/translations.csv', 'a', newline='') as f:
            writer = csv.writer(f)
            for en, zh in new_translations.items():
                writer.writerow([en, zh])
        # after renew translation, need to load translation again
        self.translations = self._load_translations()

    async def generate_comic_descriptions(self, text, chapter_num):
        """ç”Ÿæˆæ¼«ç”»æè¿°"""
        chunks = self._chunk_text(text, comic_descriptions_chunk_size)
        descriptions = []

        for chunk in chunks:
            try:
                output = await asyncio.to_thread(
                    self.summarizer,
                    chunk,
                    max_length=100,
                    min_length=50,
                    do_sample=True,
                    num_beams=4
                )
                descs = output[0]['summary_text']
                charactor_desc = contain_in_name_translation(descs, self.translations)
                descriptions.append(charactor_desc)
            except Exception as e:
                print(f"ç”Ÿæˆæè¿°å¤±è´¥: {str(e)}")

        self._save_descriptions(descriptions, chapter_num)

    def _chunk_text(self, text, chunk_size):
        """æ™ºèƒ½åˆ†å—æ–‡æœ¬"""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = []
        char_count = 0

        for sent in sentences:
            sent_len = len(sent)
            if char_count + sent_len > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                char_count = 0
            current_chunk.append(sent)
            char_count += sent_len

        if current_chunk:
            chunks.append(' '.join(current_chunk))
        return chunks

    def _chunk_polished(self, text, chunk_size):
        """æ™ºèƒ½åˆ†å—æ–‡æœ¬"""
        paragraphs = re.split(r'\n+', text)  # æŒ‰ä¸€ä¸ªæˆ–å¤šä¸ªæ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = [p.strip() for p in paragraphs]  # å»é™¤æ¯æ®µé¦–å°¾ç©ºç™½
        paragraphs = [p for p in paragraphs if p]  # è¿‡æ»¤ç©ºæ®µè½
        chunks = []
        current_para = ''
        char_count = 0

        for paragraph in paragraphs:
            sent_len = len(paragraph)
            if char_count + sent_len > chunk_size:
                chunks.append(current_para)
                current_para = ''
                char_count = 0
            if current_para == '':
                current_para = paragraph + '\n\n'
            else:
                current_para = current_para +'\n\n' + paragraph
            char_count += sent_len

        if current_para:
            chunks.append(current_para)
        return chunks

    def _save_descriptions(self, descriptions, chapter_num):
        """ä¿å­˜æ¼«ç”»æè¿°"""
        filename = f"comic_descriptions/{chapter_num}_ComicDescription.txt"
        with open(filename, 'w') as f:
            for desc in descriptions:
                f.write(f"{desc}\n")

    async def translate_content(self, text, chapter_num):
        """ç¿»è¯‘ç« èŠ‚å†…å®¹"""
        translated = ''
        paragraphs = text.split('\n\n')

        for para in paragraphs:
            if not para.strip():
                continue

            # æ›¿æ¢ä¸“æœ‰åè¯
            translated_para = para
            for en, zh in self.translations.items():
                en_no_label = split_if_contains(en, '->')
                translated_para = translated_para.replace(en_no_label, zh)
            if not has_chinese_or_english(translated_para):
                print(f'translated_para has no chinese, english or number: {translated_para}')
                continue

            # ç¿»è¯‘æ®µè½
            try:
                messages = [
                    {"role": "user",
                     "content": f"Translate the following segment into Chinese, without additional explanation.\n\n{translated_para}"},
                ]
                tokenized_chat = self.translator_tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=False,
                    return_tensors="pt"
                )

                outputs = self.translator_model.generate(tokenized_chat.to(self.translator_model.device), max_new_tokens=2048)
                output_text = self.translator_tokenizer.decode(outputs[0])
                result = process_tencent_HY(output_text)
                if 'ç¿»è¯‘' in result:
                    print('warn!!!!!!!!!!!!!!!!!!!!!!!!!!!!, å¥å­ä¸­å«æœ‰ç¿»è¯‘å­—æ ·')

                if result and len(result) > 0:
                    if translated == '':
                        translated = result
                    else:
                        translated = translated + '\n\n' + result
            except Exception as e:
                print(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
                # translated.append(para)

        self._save_translation(translated, chapter_num)


    async def translate_title(self, chapter_num):
        """ç¿»è¯‘ç« èŠ‚å†…å®¹"""
        parts = chapter_num.split(' ', 1)
        str1, str2 = parts[0], parts[1]
        try:
            messages = [
                {"role": "user",
                 "content": f"Translate the following segment into Chinese, without additional explanation.\n\n{str2}"},
            ]
            tokenized_chat = self.translator_tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=False,
                return_tensors="pt"
            )

            outputs = self.translator_model.generate(tokenized_chat.to(self.translator_model.device), max_new_tokens=2048)
            output_text = self.translator_tokenizer.decode(outputs[0])
            result = process_tencent_HY(output_text)
            if 'ç¿»è¯‘' in result:
                print('warn!!!!!!!!!!!!!!!!!!!!!!!!!!!!, å¥å­ä¸­å«æœ‰ç¿»è¯‘å­—æ ·')

            if result and len(result) > 0:
                translated_title = new_title = f"{str1} {result}.txt"
                if os.path.exists('translated/' + chapter_num + '_translated.txt'):
                    os.rename('translated/' + chapter_num + '_translated.txt', 'translated/' + translated_title)
                    print(f"æ–‡ä»¶å·²é‡å‘½åä¸º: {translated_title}")
        except Exception as e:
            print(f"titleç¿»è¯‘å¤±è´¥: {str(e)}")


    def _save_translation(self, content, chapter_num):
        """ä¿å­˜ç¿»è¯‘ç»“æœ"""
        filename = f"translated/{chapter_num}_translated.txt"
        clean_content = process_polished_text(content)
        with open(filename, 'w') as f:
            f.write(clean_content)

    async def polish_translation(self, chapter_num):
        """æ¶¦è‰²ç¿»è¯‘ç»“æœï¼ˆéœ€æ›¿æ¢ä¸ºå®é™…APIè°ƒç”¨ï¼‰"""
        input_file = f"translated/{chapter_num}"

        try:
            with open(input_file, 'r') as f:
                text = f.read()

            polished = ''
            chunks = self._chunk_polished(text, polish_chunk_size)
            for chunk in chunks:
                # è°ƒç”¨å¤§æ¨¡å‹APIï¼ˆç¤ºä¾‹ä½¿ç”¨ä¼ªä»£ç ï¼‰
                # api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
                # api_key = "sk-b12ef3f919ab4f54ae1905e645762a40", api_base = "https://api.deepseek.com/v1", model = "deepseek/deepseek-chat", max_tokens=8192
                output = await self._call_llm_api(
                    f"""
                        Please meticulously refine the following machine-translated content into polished Chinese prose that adheres to professional literary standards. The final output must:
                            only contain the refined part, do not include any other explanation
                            Maintain the original narrative intent and key details
                            Restructure awkward syntax to achieve natural flow
                            Optimize word choice using context-appropriate vocabulary
                            Enhance readability through proper pacing and paragraph transitions
                            Preserve any intentional stylistic elements from the source material
                            Eliminate translationese artifacts while ensuring semantic fidelity
                            must be Simplified Chinese Mandarin
                        Special Requirements:
                            Apply Chinese novel-writing conventions for dialogue formatting and descriptive passages
                            Verify cultural references for local relevance
                            Balance formal/informal registers according to scene context.
                        textï¼š\n{chunk}",
                    """,
                    api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
                )

                if polished == '':
                    polished = output + '\n\n'
                else:
                    polished = polished + '\n\n' + output

            clean_polished = process_polished_text(polished)

            # ç”Ÿæˆæ ‡é¢˜
            # title = await self._call_llm_api(
            #     f"Generate a contextually appropriate Chinese title for the provided content. title shall be no more than 15 charactersï¼š\n{clean_polished}",
            #     api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
            # )
            # clean_title = process_polished_text(title)
            # clean_title = re.sub(r'[\\/:*?"<>|]', '', clean_title).strip()
            # clean_title = clean_title.replace('ã€Š','').replace('ã€‹', '')
            # if len(clean_title) > 50:
            #     print('too long title!')
            #     clean_title = 'too long title'
            # ä¿å­˜ç»“æœ
            # output_file = f"polished/{chapter_num}_{clean_title}.txt"
            # with open(output_file, 'w') as f:
            #     f.write(f"\t\t{clean_title}\n\n{clean_polished}")

            output_file = f"polished/{chapter_num}"
            with open(output_file, 'w') as f:
                f.write(f"\t\t{chapter_num}\n\n{clean_polished}")

        except Exception as e:
            print(f"æ¶¦è‰²å¤±è´¥: {str(e)}")

    async def _call_llm_api(self, prompt, api_key, api_base, model, max_tokens):
        try:
            # é…ç½® litellm å‚æ•°
            response = completion(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=max_tokens,
                api_key=api_key,
                api_base=api_base
            )

            # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            return response.choices[0].message.content

        except Exception as e:
            return f"API è°ƒç”¨å¤±è´¥: {str(e)}"


async def main(chapter_name, chapter_content, processor):
    tasks = []
    print(f"\nğŸ“– æ­£åœ¨å¤„ç†ç¬¬ {chapter_name} ç« ï¼ˆ{len(chapter_content)} å­—ç¬¦ï¼‰")
    # await processor.process_entities(chapter_content)
    # tasks.append(asyncio.create_task(processor.generate_comic_descriptions(chapter_content, chapter_name)))
    # tasks.append(asyncio.create_task(processor.translate_content(chapter_content, chapter_name)))
    # tasks.append(asyncio.create_task(processor.translate_title(chapter_name)))
    tasks.append(asyncio.create_task(processor.polish_translation(chapter_name)))
    # with tqdm_asyncio(total=len(tasks), desc="å¤„ç†è¿›åº¦") as pbar:
    #     for task in asyncio.as_completed(tasks):
    #         await task
    #         pbar.update(1)

    print("\nâœ… å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨ crawl_wattpad/ ç›®å½•ä¸­")







if __name__ == "__main__":

    processor = WattpadProcessor()
    print(f'processor created: {processor}')
    folder_path = Path(raw_translated_path)
    print(folder_path)
    for file_path in sorted(folder_path.glob("*")):
        try:
            # ä½¿ç”¨utf-8ç¼–ç æ‰“å¼€æ–‡ä»¶
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
        except UnicodeDecodeError:
            # å¦‚æœutf-8è§£ç å¤±è´¥ï¼Œå°è¯•gbkç¼–ç 
            try:
                with open(file_path, "r", encoding="gbk") as f:
                    content = f.read()
            except Exception as e:
                print(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path.name}: {str(e)}")
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        asyncio.run(main(str(file_path.name), content, processor))
