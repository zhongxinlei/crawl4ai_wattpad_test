import csv
import spacy
from tqdm.asyncio import tqdm_asyncio
from transformers import pipeline
from googletrans import Translator
import torch
import os
from litellm import completion
import asyncio
from pathlib import Path
from  utuil import *

raw_novel_path = 'raw_novel_test/'
os.environ['LITELLM_LOG'] = 'DEBUG'
comic_descriptions_chunk_size = 700
polish_chunk_size = 1500
Kay_role_name = 'Jay'

class WattpadProcessor:
    def __init__(self):
        # åˆå§‹åŒ–NLPæ¨¡å‹
        self.nlp = spacy.load("en_core_web_lg")

        # åˆå§‹åŒ–ç¿»è¯‘å™¨ï¼ˆä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼‰
        self.translator = Translator()

        self.raw_translator = pipeline("translation_en_to_zh",
                     model="/home/lane/ai/models/nlp/translation/Helsinki-NLP-en-zh")

        # åˆå§‹åŒ–æ¼«ç”»æè¿°ç”Ÿæˆå™¨
        self.summarizer = pipeline(
            "summarization",
            # model="facebook/bart-large-cnn",
            model="/home/lane/ai/models/facebook/bart",
            device=1 if torch.cuda.is_available() else -1
        )

        # åˆå§‹åŒ–ç¿»è¯‘è®°å¿†åº“
        self.translations = self._load_translations()

    def _load_translations(self):
        """åŠ è½½å·²æœ‰çš„ç¿»è¯‘è®°å¿†"""
        translations = {}
        try:
            with open('names/translations.csv', 'r') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) >= 2:
                        translations[row[0]] = row[1]
        except FileNotFoundError as e:
            print(f'_load_translations error! {str(e)}')
            pass
        return translations


    async def process_entities(self, text):
        """å¤„ç†å®ä½“è¯†åˆ«å’Œç¿»è¯‘"""
        doc = self.nlp(text)
        new_translations = {}

        for ent in doc.ents:
            if ent.label_ in ['PERSON', 'GPE']:
                # æ›´æ–°ç¿»è¯‘è®°å¿†åº“
                ent_text = unicode_clean(ent.text)
                if ent_text not in self.translations:
                    zh_name = await self._translate_name(ent_text)
                    self.translations[ent_text] = zh_name
                    new_translations[ent_text] = zh_name

                # ä¿å­˜äººç‰©æè¿°
                if ent.label_ == 'PERSON':
                    await self._save_character_description(ent_text, text)

        # ä¿å­˜æ–°å¢ç¿»è¯‘
        if new_translations:
            self._save_translations(new_translations)

        return self.translations

    async def _translate_name(self, name):
        """ç¿»è¯‘ä¸“æœ‰åè¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            translated_name = await self.translator.translate(name,src='en',dest='zh-cn')
            return translated_name.text
        except Exception as e:
            print(f'_translate_name exception: {e}')
            return name

    async def _save_character_description(self, name, context):
        """ä¿å­˜äººç‰©æè¿°"""
        desc = await self._extract_description(context, name)
        text = ''
        with open('names/character_descriptions.txt', 'r') as f:
            text = f.read()
        if txt_contains_no_name_sentence(text, f'[{name}]'):
            with open('names/character_descriptions.txt', 'a') as f2:
                print(f'_save_character_description: {desc}')
                desc = desc.replace('\n', ';')
                f2.write(f"[{name}] {desc}\n")

    async def _extract_description(self, text, name):
        """æå–äººç‰©æè¿°ä¸Šä¸‹æ–‡"""
        # å®šä¹‰æç¤ºè¯

        prompt = f"""
            you are a expert in charactor extraction, please extract the detail description of {name} in the given text,
            including it's appearance, dressing and charactor.
            output shall be a full sentence and shall be less then 100 letters.
            textï¼š
            {text}
            """
        # api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
        # api_key = "sk-b12ef3f919ab4f54ae1905e645762a40", api_base = "https://api.deepseek.com/v1", model = "deepseek/deepseek-chat", max_tokens=8192
        description = await self._call_llm_api(
            prompt,
            api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
        )
        return description if description else "no desc"

    def _save_translations(self, new_translations):
        """ä¿å­˜æ–°å¢ç¿»è¯‘"""
        with open('names/translations.csv', 'a', newline='') as f:
            writer = csv.writer(f)
            for en, zh in new_translations.items():
                writer.writerow([en, zh])
        # after renew translation, need to load translation again
        self.translations = self._load_translations()

    async def generate_comic_descriptions(self, text, chapter_num):
        """ç”Ÿæˆæ¼«ç”»æè¿°"""
        chunks = self._chunk_text(text, comic_descriptions_chunk_size)
        descriptions = []

        for chunk in chunks:
            try:
                output = await asyncio.to_thread(
                    self.summarizer,
                    chunk,
                    max_length=100,
                    min_length=50,
                    do_sample=True,
                    num_beams=4
                )
                descs = output[0]['summary_text']
                charactor_desc = contain_in_name_translation(descs, self.translations)
                descriptions.append(charactor_desc)
            except Exception as e:
                print(f"ç”Ÿæˆæè¿°å¤±è´¥: {str(e)}")

        self._save_descriptions(descriptions, chapter_num)

    def _chunk_text(self, text, chunk_size):
        """æ™ºèƒ½åˆ†å—æ–‡æœ¬"""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = []
        char_count = 0

        for sent in sentences:
            sent.replace('\n', ';').replace('I ', f'{Kay_role_name} ').replace('Jayyy', f'{Kay_role_name}').replace('I\'ll', f'{Kay_role_name} will')
            sent_len = len(sent)
            if char_count + sent_len > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                char_count = 0
            current_chunk.append(sent)
            char_count += sent_len

        if current_chunk:
            chunks.append(' '.join(current_chunk))
        return chunks

    def _chunk_polished(self, text, chunk_size):
        """æ™ºèƒ½åˆ†å—æ–‡æœ¬"""
        paragraphs = re.split(r'\n+', text)  # æŒ‰ä¸€ä¸ªæˆ–å¤šä¸ªæ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = [p.strip() for p in paragraphs]  # å»é™¤æ¯æ®µé¦–å°¾ç©ºç™½
        paragraphs = [p for p in paragraphs if p]  # è¿‡æ»¤ç©ºæ®µè½
        chunks = []
        current_para = ''
        char_count = 0

        for paragraph in paragraphs:
            sent_len = len(paragraph)
            if char_count + sent_len > chunk_size:
                chunks.append(current_para)
                current_para = ''
                char_count = 0
            if current_para == '':
                current_para = paragraph + '\n\n'
            else:
                current_para = current_para +'\n\n' + paragraph
            char_count += sent_len

        if current_para:
            chunks.append(current_para)
        return chunks

    def _save_descriptions(self, descriptions, chapter_num):
        """ä¿å­˜æ¼«ç”»æè¿°"""
        filename = f"comic_descriptions/{chapter_num}_ComicDescription.txt"
        with open(filename, 'w') as f:
            for desc in descriptions:
                f.write(f"{desc}\n")

    async def translate_content(self, text, chapter_num):
        """ç¿»è¯‘ç« èŠ‚å†…å®¹"""
        translated = ''
        paragraphs = text.split('\n\n')

        for para in paragraphs:
            if not para.strip():
                continue

            # æ›¿æ¢ä¸“æœ‰åè¯
            translated_para = para
            for en, zh in self.translations.items():
                translated_para = translated_para.replace(en, zh)
            if not has_chinese_or_english(translated_para):
                print(f'translated_para has no chinese, english or number: {translated_para}')
                continue

            # ç¿»è¯‘æ®µè½
            try:
                # result = await self.translator.translate(translated_para, dest='zh-cn')
                prompt = f"""
                            you are a expert in translator from english to Chinese, please translate the given text,
                            if there is any non English in the given text, please do not translate them and keep them there without change.
                            if there is any line that can not be translated in the given text, please ignore it and return a space.
                            output must be the translated text directly, do not give anything else, do not include `<think>` part.
                            output must be Simplified Chinese Mandarin.
                            textï¼š
                            {translated_para}
                            """
                # api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
                # api_key = "sk-b12ef3f919ab4f54ae1905e645762a40", api_base = "https://api.deepseek.com/v1", model = "deepseek/deepseek-chat", max_tokens=8192
                result = await self._call_llm_api(
                    prompt,
                    api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
                )
                if 'ç¿»è¯‘' in result:
                    print('warn!!!!!!!!!!!!!!!!!!!!!!!!!!!!, å¥å­ä¸­å«æœ‰ç¿»è¯‘å­—æ ·')

                if result and len(result) > 0:
                    if translated == '':
                        translated = result
                    else:
                        translated = translated + '\n\n' + result
            except Exception as e:
                print(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
                # translated.append(para)

        self._save_translation(translated, chapter_num)

    def _save_translation(self, content, chapter_num):
        """ä¿å­˜ç¿»è¯‘ç»“æœ"""
        filename = f"translated/{chapter_num}_translated.txt"
        clean_content = process_polished_text(content)
        with open(filename, 'w') as f:
            f.write(clean_content)

    async def polish_translation(self, chapter_num):
        """æ¶¦è‰²ç¿»è¯‘ç»“æœï¼ˆéœ€æ›¿æ¢ä¸ºå®é™…APIè°ƒç”¨ï¼‰"""
        input_file = f"translated/{chapter_num}_translated.txt"

        try:
            with open(input_file, 'r') as f:
                text = f.read()

            polished = ''
            chunks = self._chunk_polished(text, polish_chunk_size)
            for chunk in chunks:
                # è°ƒç”¨å¤§æ¨¡å‹APIï¼ˆç¤ºä¾‹ä½¿ç”¨ä¼ªä»£ç ï¼‰
                # api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
                # api_key = "sk-b12ef3f919ab4f54ae1905e645762a40", api_base = "https://api.deepseek.com/v1", model = "deepseek/deepseek-chat", max_tokens=8192
                output = await self._call_llm_api(
                    f"""
                        Please meticulously refine the following machine-translated content into polished Chinese prose that adheres to professional literary standards. The final output must:
                            only contain the refined part, do not include any other explanation
                            Maintain the original narrative intent and key details
                            Restructure awkward syntax to achieve natural flow
                            Optimize word choice using context-appropriate vocabulary
                            Enhance readability through proper pacing and paragraph transitions
                            Preserve any intentional stylistic elements from the source material
                            Eliminate translationese artifacts while ensuring semantic fidelity
                            must be Simplified Chinese Mandarin
                        Special Requirements:
                            Apply Chinese novel-writing conventions for dialogue formatting and descriptive passages
                            Verify cultural references for local relevance
                            Balance formal/informal registers according to scene context.
                        textï¼š\n{chunk}",
                    """,
                    api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
                )

                if polished == '':
                    polished = output + '\n\n'
                else:
                    polished = polished + '\n\n' + output

            clean_polished = process_polished_text(polished)

            # ç”Ÿæˆæ ‡é¢˜
            title = await self._call_llm_api(
                f"Generate a contextually appropriate Chinese title for the provided content. title shall be no more than 15 charactersï¼š\n{clean_polished}",
                api_key=None, api_base="http://localhost:11434", model="ollama/llama3.3:70b", max_tokens=20000
            )
            clean_title = process_polished_text(title)
            clean_title = re.sub(r'[\\/:*?"<>|]', '', clean_title).strip()
            clean_title = clean_title.replace('ã€Š','').replace('ã€‹', '')
            # ä¿å­˜ç»“æœ
            output_file = f"polished/{chapter_num}_{clean_title}.txt"
            with open(output_file, 'w') as f:
                f.write(f"\t\t{clean_title}\n\n{clean_polished}")

        except Exception as e:
            print(f"æ¶¦è‰²å¤±è´¥: {str(e)}")

    async def _call_llm_api(self, prompt, api_key, api_base, model, max_tokens):
        try:
            # é…ç½® litellm å‚æ•°
            response = completion(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=max_tokens,
                api_key=api_key,
                api_base=api_base
            )

            # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            return response.choices[0].message.content

        except Exception as e:
            return f"API è°ƒç”¨å¤±è´¥: {str(e)}"


async def main(chapter_name, chapter_content, processor):
    tasks = []
    print(f"\nğŸ“– æ­£åœ¨å¤„ç†ç¬¬ {chapter_name} ç« ï¼ˆ{len(chapter_content)} å­—ç¬¦ï¼‰")
    # await processor.process_entities(chapter_content)
    # tasks.append(asyncio.create_task(processor.generate_comic_descriptions(chapter_content, chapter_name)))
    # tasks.append(asyncio.create_task(processor.translate_content(chapter_content, chapter_name)))
    tasks.append(asyncio.create_task(processor.polish_translation(chapter_name)))
    with tqdm_asyncio(total=len(tasks), desc="å¤„ç†è¿›åº¦") as pbar:
        for task in asyncio.as_completed(tasks):
            await task
            pbar.update(1)

    print("\nâœ… å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨ crawl_wattpad/ ç›®å½•ä¸­")


if __name__ == "__main__":
    processor = WattpadProcessor()
    print(f'processor created: {processor}')
    folder_path = Path(raw_novel_path)
    for file_path in sorted(folder_path.glob("*")):
        try:
            # ä½¿ç”¨utf-8ç¼–ç æ‰“å¼€æ–‡ä»¶
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
        except UnicodeDecodeError:
            # å¦‚æœutf-8è§£ç å¤±è´¥ï¼Œå°è¯•gbkç¼–ç 
            try:
                with open(file_path, "r", encoding="gbk") as f:
                    content = f.read()
            except Exception as e:
                print(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path.name}: {str(e)}")
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        asyncio.run(main(str(file_path.name), content, processor))
